id,title,url,plain_text
0,不到 2 小时，我自己开发出了一款聊天机器人_36氪,http://36kr.com/p/5098966.html,"账号设置我的收藏退出登录登录搜索未来汽车日报零售老板内参未来地产36Kr GlobalTech星球超人测评媒体品牌企服严选EClub创变者俱乐部EClub黄页36Kr研究院36Kr创新咨询氪榜企业服务政策汇编政府服务VClubVClub投资机构库投资人服务寻求报道寻求融资36氪Pro创业者服务开氪知识服务首页快讯资讯推荐汽车企服职场城市最新创投科技生活创新视频专题活动搜索寻求报道我要投稿寻求融资不到 2 小时，我自己开发出了一款聊天机器人chiming · 2017-10-25作为一名商人，这是一个值得下注的趋势；作为一名工程师，这是一项值得学习的技能。编者按：聊天机器人到底有没有商业价值？做起来到底麻不麻烦？本文作者Shival Gupta花费了一个月的时间去收集资料，去进行研究。最后，他花了不到两个小时做出了一个聊天机器人。并指出作为一名商人，这是一个值得下注的趋势；作为一名工程师，这是一项值得学习的技能。文章发表在hackernoon，由36氪编译。根据Flurry的一项研究，我们每天在智能手机上要花费大约5个小时。这个数据不仅令人惊讶，而且大约65%的时间（3小时15分钟）都花在了与沟通相关的活动上，比如社交媒体、短信、电子邮件和电话。这意味着，如果你去开发一个移动应用，去创业的话，就要与数百万的其他应用去争抢用户那35%的时间。而且，发现一个好的创意并且落地，也要耗费大量的成本。那么，如果不打算开发一款应用，你会怎么做呢？大多数专家认为，移动应用的下一步将会是在应用中构建一个聊天机器人。它们能够扩展Facebook或Telegram等大型社交平台的功能。而且用户不需要在手机上安装额外的应用。这是一个双赢的举措。不过，我对这个发展趋势有些怀疑。主要有以下两点：从商业角度来说，构建一个聊天机器人是毫无用处的。从工程角度来看，构建一个聊天机器人是一件疯狂的事情。为了搞清楚这个趋势背后的基本逻辑，我开始了一些研究。我花了大约一个月的时间查找资料，与专家交谈，摆弄工具包，到最后，用了大约2小时内用代码写了一个功能性的聊天机器人出来。这段旅程是我的探索之旅，我想知道我的怀疑是否是合理的，并从中获得一些乐趣。旅程开始人们普遍相信，灵感经常会在一个奇怪的时间敲开房门，所以应该要时刻保持清醒，邀请它进来。当我开始写这篇文章时，我对聊天机器人一无所知。我对人工智能的也非常肤浅。比如监督学习是如何起作用的，我都搞不清楚。甚至对于我来说，NLP只是自然语言处理（Natural Language Processing）的缩写。我研究聊天机器人，有一半的原因是为了增加我对这个趋势的了解。有句老话说得很好：抑制自己的无知是一种生活的乐趣，只有最不安分的人才会珍惜。我现在想要解决的是我对聊天机器人行业提出的第一个质疑——它们没有任何商业价值。经过一番搜寻之后，我终于找到了一个不错工具——Botlist，一个第三方数据库，罗列了许多当前在各平台用到的聊天机器人，包括电子邮件、网页、短消息服务、Slack、移动、应用等。当我带着疑问去观察聊天机器人时，我一直在猜测和思考开发者为什么会决定在自己的应用平台上开发聊天机器人，这解决了他们的什么问题？他们如何让机器人变得更有商业价值？我发现了Mitsuku，跟它聊天很有趣。Dr AI似乎也能很好地解决问题。Rightclick.io也不错，但我很难Get到它的点。通过浏览大量的聊天机器人，从Hangman到TVakis等等，他们的做法看起来似乎很实用。但它仍处于发展的初级阶段，这个趋势还没有完全成型。不过，好的一点是，我获取到了足够的灵感，从而能自己搭建一个聊天机器人，看看它们能尝试解决什么问题。在某种程度上，我感觉自己就像一个在卢浮宫漫步的艺术生，在寻找灵感。然而，有一个问题一直在困扰着我。杀手级应用在哪？现在，聊天机器人行业还处于起步阶段，从业者都在朝着一个方向去努力，去竞争——寻找下一个让聊天机器人成为主流的杀手级应用。到目前为止，在聊天机器人领域还没有哪家企业能够占据绝对的领先地位。整个行业的竞争环境非常公平。我和很多人谈过。专家们达成的共识是，在大约2~5年的时间里，我们会看到聊天机器人领域会出现真正的杀手级应用。技术还在发展，可以肯定地说，即使是现在最好的聊天机器人，也会因为人工智能和NLP等相关技术的发展而变得过时。这意味着，如果有正确的想法，任何人都可以开发出潜在的杀手级应用。这一发现非常激励人心。就目前而言，我体验过的聊天机器人做的事情似乎很简单，但从本质上讲，它们只是图形用户界面（GUI）的替代品。我特别注意到的两件事是：如果在用户完成任务需要超过2个步骤，那么聊天机器人就能提供更好的体验。如果用户只是简单的搜索和点击，聊天机器人似乎是多余的。总而言之：不管你是做一个用户界面（UI）出来还是做个聊天机器人，这都不重要。只有节省了时间，用户才会用其来解决问题。从这个角度来看，做聊天机器人确实具有商业价值。它比原来的用户界面更节省时间。我的下一段旅程是要亲自做一个聊天机器人出来，看看水到底有多深。做一个聊天机器人构建一个聊天机器人和玩拼图游戏差不多。我只需要找到合适的部件和工具来建造它。最关键的在于，要从哪里入手去做。所以，我在开始之前给定了两个目标：机器人应该像人一样说话：也就是它应该理解自然语言。机器人应该能在较麻烦的场景中发挥作用：这意味着它应该做一些，用户在原来的UI上操作超过两步才能完成的事情。有了这些限制，我在脑海中形成了一个可信度，即做一个聊天机器人出来是否困难。由于我在自然语言处理方面也是新手，可以想象我们大多数人的学习曲线是一样的。最后，我选择去做一个能够准确计算出日期的聊天机器人。例如，它会接受这样的输入，比如“从现在开始过6天，是什么时间？”“在明年9月之后再过5周，是什么时间？”简单来说，它的架构是这样的：根据一篇教程（点击），我花了不到2个小时，使用基本的NLP技术和一个基于softmax的神经网络（包括在我的Windows机器上安装Tensorflow），做出了一个意图分类器系统。在确定意图分类之后，将对字符串进行解析，以便输入我想要的日期。我宁愿用NLP模块对输入的日期进行解析，并反馈给我，但它现在只是用于概念验证。输入自然语言。结果。这个聊天机器人并不非常健谈。老实说，这并没有那么难。有了Facebook的Messenger平台和Telegram的聊天机器人平台，以及api.ai、wit.ai,、以及recast.ai等公司提供的功能，我们当中的一些人可能花更少的时间，就能做一个聊天机器人了。结语为个人的使用制作了一个聊天机器人，让我进入了一个充满各种可能性的世界。人们正在解决许多问题，比如预订旅游票、酒店、电影票、订餐等等。通过引入聊天机器人，用户体验可以通实现跨越式的改进。说实话，聊天机器人的新特性让我很兴奋。或许，新的Facebook和WhatsApp正出现在黑暗中。我相信在不远的将来，一切都可以通过像电影《Her》中的对话机器人来完成。它比你在智能手机屏幕上轻敲一下有效率多了。不过，我不会建议你爱上你的聊天机器人。抛开玩笑，聊天机器人无疑是一个新兴的趋势。每个人都在讨论它是否会持续下去。但能否持续下去并不是由聊天机器人本身来决定。而是那些制造它们的人。我相信，炒作是一种将冷门技术变得普遍化的工具。业界正在研究这一相对较新的技术能够做什么。作为一名商人，这是一个值得下注的趋势；作为一名工程师，这是一项值得学习的技能。毕竟，最好的聊天机器人还没有被制造出来。原文链接：https://hackernoon.com/i-built-a-chatbot-in-2-hours-and-this-is-what-i-learned-f5dbb4ba5fcc编译组出品。编辑：郝鹏程 本文来自翻译, 如若转载请注明出处。期待您加入36氪官方创始人社群EClub，链接有价值的创业者与投资人，让创业更简单！ 详情请戳 。+1好文章，需要你的鼓励chiming读者收  藏+1评  论打开微信“扫一扫”，打开网页后点击屏幕右上角分享按钮微  博沉浸阅读返回顶部参与评论登录后才能参与讨论哦...登录后参与讨论提交评论0/1000请回复有价值的信息，无意义的评论将很快被删除，账号将被禁止发言。chiming读者关注新产品模式。发表文章460篇最近内容这就是2100年工作的样子：女性将占据多数、等级制度将会消失2018-07-26在“孤独之国”日本，清理死者遗物成了一个热门产业2018-07-24在亚马逊的冲击下，百思买是如何逆风翻盘的？2018-07-23阅读更多内容，狠戳这里下一篇在 217：6 的屏幕上玩游戏，是一种什么样的体验？未来 Apple 以及开发者们或对 Touch Bar 这个全新的交互方式给予更多的可能性。2017-10-25关于36氪城市加盟寻求报道我要投稿投资者关系商务合作关于我们加入我们合作伙伴36氪APP下载iOS & Android本站由 阿里云 提供计算与安全服务 违法和不良信息举报电话：010-58254120 举报邮箱：jubao@36kr.com© 2011~2018 北京多氪信息科技有限公司 | 京ICP备12031756号 | 京ICP证150143号 | 京公网安备11010502036099号意见反馈36氪APP让一部分人先看到未来36氪鲸准氪空间为你推送和解读最前沿、最有料的科技创投资讯一级市场金融信息和系统服务提供商聚集全球最优秀的创业者，项目融资率接近97%，领跑行业"
1,Programming for Everybody (Getting Started with Python) | Coursera,https://www.coursera.org/learn/python?siteID=SAyYsTvLiGQ-DohbxFI_XulUJEJVnExRww&utm_content=10&utm_medium=partners&utm_source=linkshare&utm_campaign=SAyYsTvLiGQ," Loupe CopyBrowseTop CoursesFor EnterpriseLog InJoin for FreeBrowseComputer ScienceSoftware DevelopmentThis course is part of the Python for Everybody SpecializationProgramming for Everybody (Getting Started with Python)Filled StarFilled StarFilled StarFilled StarHalf Faded Star4.8stars73,533 ratingsâ¢18,009 reviewsOffered ByPython for Everybody SpecializationUniversity of MichiganAbout this Course2,205,543 recent viewsThis course aims to teach everyone the basics of programming computers using Python. We cover the basics of how one constructs a program from a series of simple instructions in Python. The course has no pre-requisites and avoids all but the simplest mathematics. Anyone with moderate computer experience should be able to master the materials in this course. This course will cover Chapters 1-5 of the textbook âPython for Everybodyâ. Once a student completes this course, they will be ready to take more advanced programming courses. This course covers Python 3.UserLearner Career OutcomesCareer direction39%started a new career after completing these coursesCareer Benefit39%got a tangible career benefit from this courseCareer promotion12%got a pay increase or promotion100% online100% onlineStart instantly and learn at your own schedule.SpecializationCourse 1 of 5 in thePython for Everybody SpecializationFlexible deadlinesFlexible deadlinesReset deadlines in accordance to your schedule.Hours to completeApprox. 12 hours to completeSuggested: 2-4 hours/week... Available languagesEnglishSubtitles: Arabic, Chinese (Simplified), Korean, German, EnglishWhat you will learnCheckDescribe the basics of the Python programming languageCheckInstall Python and write your first programCheckUse variables to store, retrieve and calculate informationCheckUtilize core programming tools such as functions and loopsSkills you will gainPython Syntax And SemanticsBasic Programming LanguageComputer ProgrammingPython ProgrammingUserLearner Career OutcomesCareer direction39%started a new career after completing these coursesCareer Benefit39%got a tangible career benefit from this courseCareer promotion12%got a pay increase or promotion100% online100% onlineStart instantly and learn at your own schedule.SpecializationCourse 1 of 5 in thePython for Everybody SpecializationFlexible deadlinesFlexible deadlinesReset deadlines in accordance to your schedule.Hours to completeApprox. 12 hours to completeSuggested: 2-4 hours/week... Available languagesEnglishSubtitles: Arabic, Chinese (Simplified), Korean, German, EnglishSyllabus - What you will learn from this courseWeek1Hours to complete2 hours to completeChapter One - Why we Program?These are the course-wide materials as well as the first part of Chapter One where we explore what it means to write programs. We finish Chapter One and have the quiz and first assignment in the third week of the class. Throughout the course you may want to come back and look at these materials. This section should not take you an entire week. Reading7 videos (Total 42 min), 6 readingsVideo7 videosVideo: Welcome to Class - Dr. Chuck4mVideo: Welcome to Python - Guido van Rossum1m1.1 - Why Program11m1.2 - Hardware Overview11m1.3 - Python as a Language7mFun: The Textbook Authors Meet @PyCon20153mFace to Face Office Hours - Bengaluru, India2mReading6 readingsReading: Welcome to The Class10mHelp Us Learn More About You!10mWelcome to Python 310mTextbook: Python for Everybody: Exploring Data in Python 310mSubmitting Assignments10mAudio Versions of All Lectures10mWeek2Hours to complete4 hours to completeInstalling and Using PythonIn this module you will set things up so you can write Python programs. Not all activities in this module are required for this class so please read the ""Using Python in this Class"" material for details.Reading6 videos (Total 33 min), 3 readings, 2 quizzesVideo6 videosDemonstration: Using the Python Playground3mWindows 10: Installing Python and Writing A Program8mWindows: Taking Screen Shots2mMacintosh: Using Python and Writing A Program4mMacintosh: Taking Screen Shots4mBonus: Eben Upton and the RaspBerry Pi9mReading3 readingsImportant Reading: Using Python in this Class10mNotes on Choice of Text Editor10mNotice for Auditing Learners: Assignment Submission10mWeek3Hours to complete2 hours to completeChapter One: Why We Program (continued)In the first chapter we try to cover the ""big picture"" of programming so you get a ""table of contents"" of the rest of the book. Don't worry if not everything makes perfect sense the first time you hear it. This chapter is quite broad and you would benefit from reading the chapter in the book in addition to watching the lectures to help it all sink in. You might want to come back and re-watch these lectures after you have funished a few more chapters.Reading4 videos (Total 37 min), 2 quizzesVideo4 videos1.4 - Writing Paragraphs of Code16mDemonstration: Doing the ""Hello World"" Assignment5mInterview: Daphne Koller - Building Coursera11mFace-to-Face Office Hours: Milan, Italy3mQuiz1 practice exerciseChapter 120mWeek4Hours to complete3 hours to completeChapter Two: Variables and ExpressionsIn this chapter we cover how a program uses the computer's memory to store, retrieve and calculate information.Reading6 videos (Total 56 min), 1 reading, 3 quizzesVideo6 videos2.1 - Expressions13m2.2 - Expressions Part 220m2.3 - Expressions - Part 37mWorked Exercise: 2.37mInterview: Pooja Sankar - Building Piazza6mOffice Hours: Mountain View, CA52sReading1 readingWhere is the worked exercise for Assignment 2.2?10mQuiz1 practice exerciseChapter 220mWeek5Hours to complete3 hours to completeChapter Three: Conditional CodeIn this section we move from sequential code that simply runs one line of code after another to conditional code where some steps are skipped. It is a very simple concept - but it is how computer software makes ""choices"".Reading5 videos (Total 57 min), 3 quizzesVideo5 videos3.1 Conditional Statements14m3.2 More Conditional Statements17mWorked Exercise: 3.29mInterview: Massimo Banzi: The Arduino11mOffice Hours: Seoul Korea4mQuiz1 practice exerciseChapter 320mWeek6Hours to complete2 hours to completeChapter Four: FunctionsThis is a relatively short chapter. We will learn about what functions are and how we can use them. The programs in the first chapters of the book are not large enough to require us to develop functions, but as the book moves into more and more complex programs, functions will be an essential way for us to make sense of our code.Reading4 videos (Total 35 min), 2 quizzesVideo4 videos4.1 - Using Functions9m4.2 - Building Functions12mInterview: Guido van Rossum: The Early Years of Python11mOffice Hours: Manila Philippines1mQuiz1 practice exerciseChapter 420mWeek7Hours to complete3 hours to completeChapter Five: Loops and IterationLoops and iteration complete our four basic programming patterns. Loops are the way we tell Python to do something over and over. Loops are the way we build programs that stay with a problem until the problem is solved.Reading8 videos (Total 67 min), 2 readings, 2 quizzesVideo8 videos5.1 - Loops and Iteration9m5.2 - Definite Loops6m5.3 - Finding the Largest Value8m5.4 - Loop Idioms18mWorked Exercise: 5.18mWhat's Next - Dr.Chuck2mInterview: Guido van Rossum - The Modern Era of Python12mOffice Hours: Paris, France52sReading2 readingsPlease Rate this Course on Class-Central10mPost-Course Survey10mQuiz1 practice exerciseChapter 520m4.8Filled StarFilled StarFilled StarFilled StarHalf Faded Star18009 ReviewsChevron RightTop reviews from Programming for Everybody (Getting Started with Python)HighlightsHigh quality instructor(1484)Introductory course(4203)Filled StarFilled StarFilled StarFilled StarFilled StarBy JTâ¢Nov 12th 2016I completed the course in about three days. I've never programmed before to the learning curve was excruciating but nonetheless I managed to get through and finally get my python feet wet so to speak.Filled StarFilled StarFilled StarFilled StarFilled StarBy BNâ¢Jan 17th 2016After trying tutorial after tutorial and exploring many different resources to learn Python, I have finally found one that works! This is a very fun course, and the free textbook is simply incredible.InstructorCharles Russell SeveranceClinical ProfessorSchool of InformationAbout University of MichiganThe mission of the University of Michigan is to serve the people of Michigan and the world through preeminence in creating, communicating, preserving and applying knowledge, art, and academic values, and in developing leaders and citizens who will challenge the present and enrich the future.... About the Python for Everybody SpecializationThis Specialization builds on the success of the Python for Everybody course and will introduce fundamental programming concepts including data structures, networked application program interfaces, and databases, using the Python programming language. In the Capstone Project, youâll use the technologies learned throughout the Specialization to design and create your own applications for data retrieval, processing, and visualization.... Frequently Asked QuestionsWhen will I have access to the lectures and assignments?Once you enroll for a Certificate, youâll have access to all videos, quizzes, and programming assignments (if applicable). Peer review assignments can only be submitted and reviewed once your session has begun. If you choose to explore the course without purchasing, you may not be able to access certain assignments.What will I get if I subscribe to this Specialization?When you enroll in the course, you get access to all of the courses in the Specialization, and you earn a certificate when you complete the work. Your electronic Certificate will be added to your Accomplishments page - from there, you can print your Certificate or add it to your LinkedIn profile. If you only want to read and view the course content, you can audit the course for free.What is the refund policy?Is financial aid available?More questions? Visit the Learner Help Center.CourseraAboutLeadershipCareersCatalogCertificatesMasterTrackâ¢ CertificatesDegreesFor EnterpriseFor GovernmentFor CampusCommunityLearnersPartnersDevelopersBeta TestersTranslatorsBlogTech BlogMoreTermsPrivacyHelpAccessibilityPressContactDirectoryAffiliatesLearn AnywhereFollow UsÂ© 2020 Coursera Inc. All rights reserved."
2,機器學習 百日馬拉松 - 首頁,https://ai100.cupoy.com/,You need to enable JavaScript to run this app.
3,A Game-Engine-Based Learning Environment Framework for Artificial General Intelligence | SpringerLink,https://link.springer.com/chapter/10.1007/978-3-319-46687-3_39,"Skip to main contentThis service is more advanced with JavaScript available, learn more at http://activatejavascript.orgAdvertisementSpringerLinkNeural Information Processing  International Conference on Neural Information ProcessingICONIP 2016: Neural Information Processing pp 351-356 | Cite asA Game-Engine-Based Learning Environment Framework for Artificial General IntelligenceToward Democratic AGIAuthorsAuthors and affiliationsMasayoshi NakamuraHiroshi YamakawaConference paperFirst Online: 29 September 2016 1Readers1.6kDownloads Part of the Lecture Notes in Computer Science book series (LNCS, volume 9947)AbstractArtificial General Intelligence (AGI) refers to machine intelligence that can effectively conduct variety of human tasks. Therefore AGI research requires multivariate and realistic learning environments. In recent years, game engines capable of constructing highly realistic 3D virtual worlds have also become available at low cost. In accordance with these changes, we developed the “Life in Silico” (LIS) framework, which provides virtual agents with learning algorithms and their learning environments with game engine. This should in turn allow for easier and more flexible AGI research. Furthermore, non-experts will be able to play with the framework, which would enable them to research as their hobby. If AGI research becomes popular in this manner, we may see a sudden acceleration towards the “Democratization of AGI”.KeywordsArtificial general intelligence Simulation-Based learning environment Machine learning This is a preview of subscription content, log in to check access.NotesAcknowledgementsThanks to all members of the WBAI and the members of the WBA Future Leaders.References1.Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T.P., Harley, T., Silver, D., Kavukcuoglu, K.: Asynchronous methods for deep reinforcement learning (2016). arXiv preprint arXiv:1602.017832.Lerer, A., Gross, S., Fergus, R.: Learning physical intuition of block towers by example (2016). arXiv preprint arXiv:1603.013123.Abel, D., et al.: Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains (2016). arXiv preprint arXiv:1603.013124.Krizhevsky, A., Sutskever, I., Hinton, G.: ImageNet classification with deep convolutional neural networks. In: NIPS (2012)Google Scholar5.Mnih, V., et al.: Humanlevel control through deep reinforcement learning. Nature 518(7540), 529–533 (2015)CrossRefGoogle ScholarCopyright information© Springer International Publishing AG 2016Authors and AffiliationsMasayoshi Nakamura12Email authorHiroshi Yamakawa121.Dwango Artificial Intelligence LaboratoryDWANGO Co., Ltd.Chuo-KuJapan2.The Whole Brain Architecture InitiativeChuo-KuJapanAbout this paper Cite this paper as: Nakamura M., Yamakawa H. (2016) A Game-Engine-Based Learning Environment Framework for Artificial General Intelligence. In: Hirose A., Ozawa S., Doya K., Ikeda K., Lee M., Liu D. (eds) Neural Information Processing. ICONIP 2016. Lecture Notes in Computer Science, vol 9947. Springer, ChamFirst Online29 September 2016DOIhttps://doi.org/10.1007/978-3-319-46687-3_39Publisher NameSpringer, ChamPrint ISBN978-3-319-46686-6Online ISBN978-3-319-46687-3eBook PackagesComputer ScienceBuy this book on publisher's siteReprints and PermissionsPersonalised recommendationsCitepaperHow to cite? .RIS  Papers  Reference Manager  RefWorks  Zotero  .ENW  EndNote  .BIB  BibTeX  JabRef  Mendeley  Buy options ActionsLog in to check accessBuy eBook EUR 74.89  EUR 24.95 Instant downloadReadable on all devicesOwn it foreverLocal sales tax included if applicable Learn about institutional subscriptionsCitepaperHow to cite? .RIS  Papers  Reference Manager  RefWorks  Zotero  .ENW  EndNote  .BIB  BibTeX  JabRef  Mendeley AdvertisementOver 10 million scientific documents at your fingertipsSwitch EditionAcademic EditionCorporate EditionHomeImpressumLegal informationPrivacy statementHow we use cookiesCookie settingsAccessibilityContact usSpringer Nature© 2019 Springer Nature Switzerland AG. Part of Springer Nature.Not logged inNot affiliated27.247.102.134"
4,Whole brain connectomic architecture to develop general artificial intelligence - ScienceDirect,https://www.sciencedirect.com/science/article/pii/S1877050918300498,"Skip to main contentThis service is more advanced with JavaScript available, learn more at http://activatejavascript.orgAdvertisementSpringerLinkNeural Information Processing  International Conference on Neural Information ProcessingICONIP 2016: Neural Information Processing pp 351-356 | Cite asA Game-Engine-Based Learning Environment Framework for Artificial General IntelligenceToward Democratic AGIAuthorsAuthors and affiliationsMasayoshi NakamuraHiroshi YamakawaConference paperFirst Online: 29 September 2016 1Readers1.6kDownloads Part of the Lecture Notes in Computer Science book series (LNCS, volume 9947)AbstractArtificial General Intelligence (AGI) refers to machine intelligence that can effectively conduct variety of human tasks. Therefore AGI research requires multivariate and realistic learning environments. In recent years, game engines capable of constructing highly realistic 3D virtual worlds have also become available at low cost. In accordance with these changes, we developed the “Life in Silico” (LIS) framework, which provides virtual agents with learning algorithms and their learning environments with game engine. This should in turn allow for easier and more flexible AGI research. Furthermore, non-experts will be able to play with the framework, which would enable them to research as their hobby. If AGI research becomes popular in this manner, we may see a sudden acceleration towards the “Democratization of AGI”.KeywordsArtificial general intelligence Simulation-Based learning environment Machine learning This is a preview of subscription content, log in to check access.NotesAcknowledgementsThanks to all members of the WBAI and the members of the WBA Future Leaders.References1.Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T.P., Harley, T., Silver, D., Kavukcuoglu, K.: Asynchronous methods for deep reinforcement learning (2016). arXiv preprint arXiv:1602.017832.Lerer, A., Gross, S., Fergus, R.: Learning physical intuition of block towers by example (2016). arXiv preprint arXiv:1603.013123.Abel, D., et al.: Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains (2016). arXiv preprint arXiv:1603.013124.Krizhevsky, A., Sutskever, I., Hinton, G.: ImageNet classification with deep convolutional neural networks. In: NIPS (2012)Google Scholar5.Mnih, V., et al.: Humanlevel control through deep reinforcement learning. Nature 518(7540), 529–533 (2015)CrossRefGoogle ScholarCopyright information© Springer International Publishing AG 2016Authors and AffiliationsMasayoshi Nakamura12Email authorHiroshi Yamakawa121.Dwango Artificial Intelligence LaboratoryDWANGO Co., Ltd.Chuo-KuJapan2.The Whole Brain Architecture InitiativeChuo-KuJapanAbout this paper Cite this paper as: Nakamura M., Yamakawa H. (2016) A Game-Engine-Based Learning Environment Framework for Artificial General Intelligence. In: Hirose A., Ozawa S., Doya K., Ikeda K., Lee M., Liu D. (eds) Neural Information Processing. ICONIP 2016. Lecture Notes in Computer Science, vol 9947. Springer, ChamFirst Online29 September 2016DOIhttps://doi.org/10.1007/978-3-319-46687-3_39Publisher NameSpringer, ChamPrint ISBN978-3-319-46686-6Online ISBN978-3-319-46687-3eBook PackagesComputer ScienceBuy this book on publisher's siteReprints and PermissionsPersonalised recommendationsCitepaperHow to cite? .RIS  Papers  Reference Manager  RefWorks  Zotero  .ENW  EndNote  .BIB  BibTeX  JabRef  Mendeley  Buy options ActionsLog in to check accessBuy eBook EUR 74.89  EUR 24.95 Instant downloadReadable on all devicesOwn it foreverLocal sales tax included if applicable Learn about institutional subscriptionsCitepaperHow to cite? .RIS  Papers  Reference Manager  RefWorks  Zotero  .ENW  EndNote  .BIB  BibTeX  JabRef  Mendeley AdvertisementOver 10 million scientific documents at your fingertipsSwitch EditionAcademic EditionCorporate EditionHomeImpressumLegal informationPrivacy statementHow we use cookiesCookie settingsAccessibilityContact usSpringer Nature© 2019 Springer Nature Switzerland AG. Part of Springer Nature.Not logged inNot affiliated27.247.102.134"
5,ShareCourse 學聯網,https://www.sharecourse.net/sharecourse/course/content/homepage/1700,"Skip to main contentThis service is more advanced with JavaScript available, learn more at http://activatejavascript.orgAdvertisementSpringerLinkNeural Information Processing  International Conference on Neural Information ProcessingICONIP 2016: Neural Information Processing pp 351-356 | Cite asA Game-Engine-Based Learning Environment Framework for Artificial General IntelligenceToward Democratic AGIAuthorsAuthors and affiliationsMasayoshi NakamuraHiroshi YamakawaConference paperFirst Online: 29 September 2016 1Readers1.6kDownloads Part of the Lecture Notes in Computer Science book series (LNCS, volume 9947)AbstractArtificial General Intelligence (AGI) refers to machine intelligence that can effectively conduct variety of human tasks. Therefore AGI research requires multivariate and realistic learning environments. In recent years, game engines capable of constructing highly realistic 3D virtual worlds have also become available at low cost. In accordance with these changes, we developed the “Life in Silico” (LIS) framework, which provides virtual agents with learning algorithms and their learning environments with game engine. This should in turn allow for easier and more flexible AGI research. Furthermore, non-experts will be able to play with the framework, which would enable them to research as their hobby. If AGI research becomes popular in this manner, we may see a sudden acceleration towards the “Democratization of AGI”.KeywordsArtificial general intelligence Simulation-Based learning environment Machine learning This is a preview of subscription content, log in to check access.NotesAcknowledgementsThanks to all members of the WBAI and the members of the WBA Future Leaders.References1.Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T.P., Harley, T., Silver, D., Kavukcuoglu, K.: Asynchronous methods for deep reinforcement learning (2016). arXiv preprint arXiv:1602.017832.Lerer, A., Gross, S., Fergus, R.: Learning physical intuition of block towers by example (2016). arXiv preprint arXiv:1603.013123.Abel, D., et al.: Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains (2016). arXiv preprint arXiv:1603.013124.Krizhevsky, A., Sutskever, I., Hinton, G.: ImageNet classification with deep convolutional neural networks. In: NIPS (2012)Google Scholar5.Mnih, V., et al.: Humanlevel control through deep reinforcement learning. Nature 518(7540), 529–533 (2015)CrossRefGoogle ScholarCopyright information© Springer International Publishing AG 2016Authors and AffiliationsMasayoshi Nakamura12Email authorHiroshi Yamakawa121.Dwango Artificial Intelligence LaboratoryDWANGO Co., Ltd.Chuo-KuJapan2.The Whole Brain Architecture InitiativeChuo-KuJapanAbout this paper Cite this paper as: Nakamura M., Yamakawa H. (2016) A Game-Engine-Based Learning Environment Framework for Artificial General Intelligence. In: Hirose A., Ozawa S., Doya K., Ikeda K., Lee M., Liu D. (eds) Neural Information Processing. ICONIP 2016. Lecture Notes in Computer Science, vol 9947. Springer, ChamFirst Online29 September 2016DOIhttps://doi.org/10.1007/978-3-319-46687-3_39Publisher NameSpringer, ChamPrint ISBN978-3-319-46686-6Online ISBN978-3-319-46687-3eBook PackagesComputer ScienceBuy this book on publisher's siteReprints and PermissionsPersonalised recommendationsCitepaperHow to cite? .RIS  Papers  Reference Manager  RefWorks  Zotero  .ENW  EndNote  .BIB  BibTeX  JabRef  Mendeley  Buy options ActionsLog in to check accessBuy eBook EUR 74.89  EUR 24.95 Instant downloadReadable on all devicesOwn it foreverLocal sales tax included if applicable Learn about institutional subscriptionsCitepaperHow to cite? .RIS  Papers  Reference Manager  RefWorks  Zotero  .ENW  EndNote  .BIB  BibTeX  JabRef  Mendeley AdvertisementOver 10 million scientific documents at your fingertipsSwitch EditionAcademic EditionCorporate EditionHomeImpressumLegal informationPrivacy statementHow we use cookiesCookie settingsAccessibilityContact usSpringer Nature© 2019 Springer Nature Switzerland AG. Part of Springer Nature.Not logged inNot affiliated27.247.102.134"
6,英雄集結：深度學習的魔法使們,https://ithelp.ithome.com.tw/users/20112540/ironman/2064?sc=iThelpR," 莉森揪 (alisonyang) iT邦新手 5 級 ‧ 點數  205 21206累計瀏覽數57人在追蹤站內簡訊 追蹤  個人背景 0發問 31文章 0回答 0邀請回答 0最佳解答 其他 鐵人檔案收藏的問題收藏的文章追蹤的問題追蹤的文章追蹤的tag追蹤的邦友Like的發問Like的回答Like的文章Like的留言問答討論問答回應文章留言文章回應聊天室訂閱列表 鐵人檔案  2019 iT 邦幫忙鐵人賽 回列表 AI & Data  英雄集結：深度學習的魔法使們 系列深度學習就像是個幻境之地，許多人曾經尋訪卻只得到碎片般的景色。在這裡，希望讓想成為深度學習的魔法使玩家能一覽深度學習之全貌，在30天後可以自信的說出：「沒錯，請你叫我『大魔法使』」。內容將包含深度學習的Learning map、最新研究技術與商業應用。  鐵人鍊成 ｜ 共 31 篇文章 ｜124 人訂閱 訂閱系列文 RSS系列文 4Like0留言6612瀏覽DAY 1  達標好文 [序幕] AI（人工智慧）、Machine Learning（機器學習）、 Deep Learning（深度學習）是什麼？  2018年堪稱是台灣的「AI 元年」，政府推動產業 AI 化，同時也不遺餘力的培養 AI 種子們。相信不管是在新聞媒體上或是公司內部都可常看到或聽到「AI」字眼... 2018-10-16 ‧ 由 莉森揪  分享 5Like1留言5287瀏覽DAY 2  達標好文 [地圖] 深度學習世界的魔法陣們  剛開始研究 deep learning 時，正好是 AlphaGo 跟南韓棋士李世乭對戰(2016年3月8日到3月15日)的前一兩個月，那時我們小組嘗試用 CN... 2018-10-17 ‧ 由 莉森揪  分享 4Like0留言5071瀏覽DAY 3  [魔法陣系列] Artificial Neural Network (ANN) 之術式解析  第一個魔法陣：Artificial Neural Network (ANN, 1943)首先先來看看 ANN 的結構：圖片來源：https://hack... 2018-10-18 ‧ 由 莉森揪  分享 4Like0留言4816瀏覽DAY 4  [魔法陣系列] Artificial Neural Network (ANN) 之術式啟動  上篇介紹 ANN 魔法陣結構：輸入層（Input Layer）、隱藏層（Hidden Layer）及輸出層（Output Layer）。此外，也解釋了神經元與激... 2018-10-19 ‧ 由 莉森揪  分享 4Like0留言3328瀏覽DAY 5  [實戰系列] 使用 TensorFlow 搭建一個 ANN 魔法陣（模型）  有了先前的 ANN 魔法陣教學後，該是來讓各位見習魔法使實戰演練了，前情提要請參見：[魔法陣系列] Artificial Neural Network (A... 2018-10-20 ‧ 由 莉森揪  分享 4Like1留言9775瀏覽DAY 6  [精進魔法] Regularization：減少 Overfitting ，提高模型泛化能力   當開始興致勃勃的嘗試畫魔法陣，搭建神經網絡模型時，也許會遇到下面的情形：哥布林之吶喊：我明明在訓練集表現很好啊，為什麼實際上線時結果卻崩潰了（抱頭）那你... 2018-10-21 ‧ 由 莉森揪  分享 3Like0留言3330瀏覽DAY 7  [精進魔法] Optimization：優化深度學習模型的技巧（上）  上篇提到怎麼避免 Overfitting 的技巧，本文要帶給大家的是如何優化深度學習，提高模型的效能。Batch & Mini batch深度學習每... 2018-10-22 ‧ 由 莉森揪  分享 5Like0留言6055瀏覽DAY 8  [精進魔法] Optimization：優化深度學習模型的技巧（中）－ Adaptive Learning Rates  前情提要在 [精進魔法] Optimization：優化深度學習模型的技巧（上）一文中提及了下面三種優化 deep learning 模型的作法：Batc... 2018-10-23 ‧ 由 莉森揪  分享 3Like0留言5140瀏覽DAY 9  [精進魔法] Optimization：優化深度學習模型的技巧（下）－ Batch Normalization  本文主題是「Batch Normalization」，Ian Goodfellow 大大在《Deep Learning》一書中是這麼描述 Batch Norma... 2018-10-24 ‧ 由 莉森揪  分享 3Like0留言1957瀏覽DAY 10  [魔王出沒] 深度學習中的魔王軍簡介  其實這篇應該先寫於《精進魔法》系列之前的，但沒關係，只要有 [地圖] 深度學習世界的魔法陣們 指引，你能夠照你想要挑戰的項目去學習。以下介紹深度學習的魔王們，... 2018-10-25 ‧ 由 莉森揪  分享 上一頁1234下一頁莉森揪的鐵人檔案莉森揪的收藏 收藏的問題收藏的文章莉森揪的追蹤 追蹤的問題追蹤的文章追蹤的tag追蹤的邦友莉森揪的Like Like的發問Like的回答Like的文章Like的留言莉森揪的紀錄 問答討論問答回應文章留言文章回應聊天室莉森揪的訂閱列表電週文化事業版權所有、轉載必究 | Copyright © iThome刊登廣告授權服務服務信箱隱私權聲明與會員使用條款iT邦幫忙使用說明At輸入對方的帳號或暱稱 Loading  找不到結果。 標記{{ result.label }}{{ result.account }}關閉"
7,超簡單用Python預測股價 | FinLab 量化實驗室,https://www.finlab.tw/%E8%B6%85%E7%B0%A1%E5%96%AE-Machine-Learning-%E9%A0%90%E6%B8%AC%E8%82%A1%E5%83%B9/," 莉森揪 (alisonyang) iT邦新手 5 級 ‧ 點數  205 21206累計瀏覽數57人在追蹤站內簡訊 追蹤  個人背景 0發問 31文章 0回答 0邀請回答 0最佳解答 其他 鐵人檔案收藏的問題收藏的文章追蹤的問題追蹤的文章追蹤的tag追蹤的邦友Like的發問Like的回答Like的文章Like的留言問答討論問答回應文章留言文章回應聊天室訂閱列表 鐵人檔案  2019 iT 邦幫忙鐵人賽 回列表 AI & Data  英雄集結：深度學習的魔法使們 系列深度學習就像是個幻境之地，許多人曾經尋訪卻只得到碎片般的景色。在這裡，希望讓想成為深度學習的魔法使玩家能一覽深度學習之全貌，在30天後可以自信的說出：「沒錯，請你叫我『大魔法使』」。內容將包含深度學習的Learning map、最新研究技術與商業應用。  鐵人鍊成 ｜ 共 31 篇文章 ｜124 人訂閱 訂閱系列文 RSS系列文 4Like0留言6612瀏覽DAY 1  達標好文 [序幕] AI（人工智慧）、Machine Learning（機器學習）、 Deep Learning（深度學習）是什麼？  2018年堪稱是台灣的「AI 元年」，政府推動產業 AI 化，同時也不遺餘力的培養 AI 種子們。相信不管是在新聞媒體上或是公司內部都可常看到或聽到「AI」字眼... 2018-10-16 ‧ 由 莉森揪  分享 5Like1留言5287瀏覽DAY 2  達標好文 [地圖] 深度學習世界的魔法陣們  剛開始研究 deep learning 時，正好是 AlphaGo 跟南韓棋士李世乭對戰(2016年3月8日到3月15日)的前一兩個月，那時我們小組嘗試用 CN... 2018-10-17 ‧ 由 莉森揪  分享 4Like0留言5071瀏覽DAY 3  [魔法陣系列] Artificial Neural Network (ANN) 之術式解析  第一個魔法陣：Artificial Neural Network (ANN, 1943)首先先來看看 ANN 的結構：圖片來源：https://hack... 2018-10-18 ‧ 由 莉森揪  分享 4Like0留言4816瀏覽DAY 4  [魔法陣系列] Artificial Neural Network (ANN) 之術式啟動  上篇介紹 ANN 魔法陣結構：輸入層（Input Layer）、隱藏層（Hidden Layer）及輸出層（Output Layer）。此外，也解釋了神經元與激... 2018-10-19 ‧ 由 莉森揪  分享 4Like0留言3328瀏覽DAY 5  [實戰系列] 使用 TensorFlow 搭建一個 ANN 魔法陣（模型）  有了先前的 ANN 魔法陣教學後，該是來讓各位見習魔法使實戰演練了，前情提要請參見：[魔法陣系列] Artificial Neural Network (A... 2018-10-20 ‧ 由 莉森揪  分享 4Like1留言9775瀏覽DAY 6  [精進魔法] Regularization：減少 Overfitting ，提高模型泛化能力   當開始興致勃勃的嘗試畫魔法陣，搭建神經網絡模型時，也許會遇到下面的情形：哥布林之吶喊：我明明在訓練集表現很好啊，為什麼實際上線時結果卻崩潰了（抱頭）那你... 2018-10-21 ‧ 由 莉森揪  分享 3Like0留言3330瀏覽DAY 7  [精進魔法] Optimization：優化深度學習模型的技巧（上）  上篇提到怎麼避免 Overfitting 的技巧，本文要帶給大家的是如何優化深度學習，提高模型的效能。Batch & Mini batch深度學習每... 2018-10-22 ‧ 由 莉森揪  分享 5Like0留言6055瀏覽DAY 8  [精進魔法] Optimization：優化深度學習模型的技巧（中）－ Adaptive Learning Rates  前情提要在 [精進魔法] Optimization：優化深度學習模型的技巧（上）一文中提及了下面三種優化 deep learning 模型的作法：Batc... 2018-10-23 ‧ 由 莉森揪  分享 3Like0留言5140瀏覽DAY 9  [精進魔法] Optimization：優化深度學習模型的技巧（下）－ Batch Normalization  本文主題是「Batch Normalization」，Ian Goodfellow 大大在《Deep Learning》一書中是這麼描述 Batch Norma... 2018-10-24 ‧ 由 莉森揪  分享 3Like0留言1957瀏覽DAY 10  [魔王出沒] 深度學習中的魔王軍簡介  其實這篇應該先寫於《精進魔法》系列之前的，但沒關係，只要有 [地圖] 深度學習世界的魔法陣們 指引，你能夠照你想要挑戰的項目去學習。以下介紹深度學習的魔王們，... 2018-10-25 ‧ 由 莉森揪  分享 上一頁1234下一頁莉森揪的鐵人檔案莉森揪的收藏 收藏的問題收藏的文章莉森揪的追蹤 追蹤的問題追蹤的文章追蹤的tag追蹤的邦友莉森揪的Like Like的發問Like的回答Like的文章Like的留言莉森揪的紀錄 問答討論問答回應文章留言文章回應聊天室莉森揪的訂閱列表電週文化事業版權所有、轉載必究 | Copyright © iThome刊登廣告授權服務服務信箱隱私權聲明與會員使用條款iT邦幫忙使用說明At輸入對方的帳號或暱稱 Loading  找不到結果。 標記{{ result.label }}{{ result.account }}關閉"
8,"junyanz/CycleGAN: Software that can generate photos from paintings, turn horses into zebras, perform style transfer, and more.",https://github.com/junyanz/CycleGAN,"Skip to content In this repository  All GitHub ↵ Jump to ↵No suggested jump to results In this repository  All GitHub ↵ Jump to ↵ In this repository  All GitHub ↵ Jump to ↵  Sign in  Sign up  Watch  393  Star 9.4k  Fork 1.6k junyanz/CycleGAN Join GitHub todayGitHub is home to over 40 million developers working together to host and review code, manage projects, and build software together.Sign up  Software that can generate photos from paintings, turn horses into zebras, perform style transfer, and more.   gan generative-adversarial-network deep-learning image-generation image-manipulation cyclegan pix2pix gans computer-vision computer-graphics torch 95  commits  1  branch  0  packages  0  releases  Fetching contributors   View license LuaPythonShellTeXLua94.7%Python3.2%Shell1.6%TeX0.5%Branch:masterFind file Clone or download   Clone with HTTPS  Use Git or checkout with SVN using the web URL. Download ZIPDownloading Want to be notified of new releases in junyanz/CycleGAN? Sign inSign upLaunching GitHub DesktopIf nothing happens, download GitHub Desktop and try again.Launching GitHub DesktopIf nothing happens, download GitHub Desktop and try again.Launching XcodeIf nothing happens, download Xcode and try again.Launching Visual StudioIf nothing happens, download the GitHub extension for Visual Studio and try again. taesungpfixed typoLatest commit 253a5a6 Aug 12, 2019PermalinkTypeNameLatest commit messageCommit timeFailed to load latest commit information.datadatasetsremoved cityscapes from the list of hosted datasets.Aug 13, 2019examplesadd maps training script to examples/Jun 18, 2017imgsadd a failure case imageApr 11, 2017modelsrename identity -> lambda_identityFeb 19, 2018pretrained_modelsSupport for keeping the original aspect ratio in one_direction_test_m…Apr 4, 2017utilUpdate cudnn_convert_custom.luaMar 20, 2018.gitignoreupdate READMEJun 12, 2017LICENSEfirst commitMar 30, 2017README.mdfixed typoAug 13, 2019options.luaUpdate options.luaFeb 19, 2018test.luafix display_id bugApr 8, 2017train.luafix save_current_resultsJun 18, 2017 README.md CycleGANPyTorch | project page | paperTorch implementation for learning an image-to-image translation (i.e. pix2pix) without input-output pairs, for example:Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial NetworksJun-Yan Zhu*, Taesung Park*, Phillip Isola, Alexei A. EfrosBerkeley AI Research Lab, UC BerkeleyIn ICCV 2017. (* equal contributions)This package includes CycleGAN, pix2pix, as well as other methods like BiGAN/ALI and Apple's paper S+U learning.The code was written by Jun-Yan Zhu and Taesung Park.Update: Please check out PyTorch implementation for CycleGAN and pix2pix.The PyTorch version is under active development and can produce results comparable or better than this Torch version.Other implementations: [Tensorflow] (by Harry Yang),[Tensorflow] (by Archit Rathore),[Tensorflow] (by Van Huy),[Tensorflow] (by Xiaowei Hu),  [Tensorflow-simple] (by Zhenliang He), [TensorLayer] (by luoxier),[Chainer] (by Yanghua Jin),[Minimal PyTorch] (by yunjey),[Mxnet] (by Ldpe2G),[lasagne/Keras] (by tjwei), [Keras] (by Simon Karlsson)ApplicationsMonet Paintings to PhotosCollection Style TransferObject TransfigurationSeason TransferPhoto Enhancement: Narrow depth of fieldPrerequisitesLinux or OSXNVIDIA GPU + CUDA CuDNN (CPU mode and CUDA without CuDNN may work with minimal modification, but untested)For MAC users, you need the Linux/GNU commands gfind and gwc, which can be installed with brew install findutils coreutils.Getting StartedInstallationInstall torch and dependencies from https://github.com/torch/distroInstall torch packages nngraph, class, displayluarocks install nngraphluarocks install classluarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspecClone this repo:git clone https://github.com/junyanz/CycleGANcd CycleGANApply a Pre-trained ModelDownload the test photos (taken by Alexei Efros):bash ./datasets/download_dataset.sh ae_photosDownload the pre-trained model style_cezanne (For CPU model, use style_cezanne_cpu):bash ./pretrained_models/download_model.sh style_cezanneNow, let's generate Paul Cézanne style images:DATA_ROOT=./datasets/ae_photos name=style_cezanne_pretrained model=one_direction_test phase=test loadSize=256 fineSize=256 resize_or_crop=""scale_width"" th test.luaThe test results will be saved to ./results/style_cezanne_pretrained/latest_test/index.html.Please refer to Model Zoo for more pre-trained models../examples/test_vangogh_style_on_ae_photos.sh is an example script that downloads the pretrained Van Gogh style network and runs it on Efros's photos.TrainDownload a dataset (e.g. zebra and horse images from ImageNet):bash ./datasets/download_dataset.sh horse2zebraTrain a model:DATA_ROOT=./datasets/horse2zebra name=horse2zebra_model th train.lua(CPU only) The same training command without using a GPU or CUDNN. Setting the environment variables gpu=0 cudnn=0 forces CPU onlyDATA_ROOT=./datasets/horse2zebra name=horse2zebra_model gpu=0 cudnn=0 th train.lua(Optionally) start the display server to view results as the model trains. (See Display UI for more details):th -ldisplay.start 8000 0.0.0.0TestFinally, test the model:DATA_ROOT=./datasets/horse2zebra name=horse2zebra_model phase=test th test.luaThe test results will be saved to an HTML file here: ./results/horse2zebra_model/latest_test/index.html.Model ZooDownload the pre-trained models with the following script. The model will be saved to ./checkpoints/model_name/latest_net_G.t7.bash ./pretrained_models/download_model.sh model_nameorange2apple (orange -> apple) and apple2orange: trained on ImageNet categories apple and orange.horse2zebra (horse -> zebra) and zebra2horse (zebra -> horse): trained on ImageNet categories horse and zebra.style_monet (landscape photo -> Monet painting style), style_vangogh (landscape photo -> Van Gogh painting style), style_ukiyoe (landscape photo -> Ukiyo-e painting style), style_cezanne (landscape photo -> Cezanne painting style): trained on paintings and Flickr landscape photos.monet2photo (Monet paintings -> real landscape): trained on paintings and Flickr landscape photographs.cityscapes_photo2label (street scene -> label) and cityscapes_label2photo (label -> street scene): trained on the Cityscapes dataset.map2sat (map -> aerial photo) and sat2map (aerial photo -> map): trained on Google maps.iphone2dslr_flower (iPhone photos of flowers -> DSLR photos of flowers): trained on Flickr photos.CPU models can be downloaded using:bash pretrained_models/download_model.sh <name>_cpu, where <name> can be horse2zebra, style_monet, etc. You just need to append _cpu to the target model.Training and Test DetailsTo train a model,DATA_ROOT=/path/to/data/ name=expt_name th train.luaModels are saved to ./checkpoints/expt_name (can be changed by passing checkpoint_dir=your_dir in train.lua).See opt_train in options.lua for additional training options.To test the model,DATA_ROOT=/path/to/data/ name=expt_name phase=test th test.luaThis will run the model named expt_name in both directions on all images in /path/to/data/testA and /path/to/data/testB.A webpage with result images will be saved to ./results/expt_name (can be changed by passing results_dir=your_dir in test.lua).See opt_test in options.lua for additional test options. Please use model=one_direction_test if you only would like to generate outputs of the trained network in only one direction, and specify which_direction=AtoB or which_direction=BtoA to set the direction.There are other options that can be used. For example, you can specify resize_or_crop=crop option to avoid resizing the image to squares. This is indeed how we trained GTA2Cityscapes model in the projet webpage and Cycada model. We prepared the images at 1024px resolution, and used resize_or_crop=crop fineSize=360 to work with the cropped images of size 360x360. We also used lambda_identity=1.0.DatasetsDownload the datasets using the following script. Many of the datasets were collected by other researchers. Please cite their papers if you use the data.bash ./datasets/download_dataset.sh dataset_namefacades: 400 images from the CMP Facades dataset. [Citation]cityscapes: 2975 images from the Cityscapes training set. [Citation]. Note: Due to license issue, we do not host the dataset on our repo. Please download the dataset directly from the Cityscapes webpage. Please refer to ./datasets/prepare_cityscapes_dataset.py for more detail.maps: 1096 training images scraped from Google Maps.horse2zebra: 939 horse images and 1177 zebra images downloaded from ImageNet using the keywords wild horse and zebraapple2orange: 996 apple images and 1020 orange images downloaded from ImageNet using the keywords apple and navel orange.summer2winter_yosemite: 1273 summer Yosemite images and 854 winter Yosemite images were downloaded using Flickr API. See more details in our paper.monet2photo, vangogh2photo, ukiyoe2photo, cezanne2photo: The art images were downloaded from Wikiart. The real photos are downloaded from Flickr using the combination of the tags landscape and landscapephotography. The training set size of each class is Monet:1074, Cezanne:584, Van Gogh:401, Ukiyo-e:1433, Photographs:6853.iphone2dslr_flower: both classes of images were downloaded from Flickr. The training set size of each class is iPhone:1813, DSLR:3316. See more details in our paper.Display UIOptionally, for displaying images during training and test, use the display package.Install it with: luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspecThen start the server with: th -ldisplay.startOpen this URL in your browser: http://localhost:8000By default, the server listens on localhost. Pass 0.0.0.0 to allow external connections on any interface:th -ldisplay.start 8000 0.0.0.0Then open http://(hostname):(port)/ in your browser to load the remote desktop.Setup Training and Test dataTo train CycleGAN model on your own datasets, you need to create a data folder with two subdirectories trainA and trainB that contain images from domain A and B. You can test your model on your training set by setting phase='train' in test.lua. You can also create subdirectories testA and testB if you have test data.You should not expect our method to work on just any random combination of input and output datasets (e.g. cats<->keyboards). From our experiments, we find it works better if two datasets share similar visual content. For example, landscape painting<->landscape photographs works much better than portrait painting <-> landscape photographs. zebras<->horses achieves compelling results while cats<->dogs completely fails. See the following section for more discussion.Failure casesOur model does not work well when the test image is rather different from the images on which the model is trained, as is the case in the figure to the left (we trained on horses and zebras without riders, but test here one a horse with a rider). See additional typical failure cases here. On translation tasks that involve color and texture changes, like many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of dog<->cat transfiguration, the learned translation degenerates into making minimal changes to the input. We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method. In some cases, this gap may be very hard -- or even impossible,-- to close: for example, our method sometimes permutes the labels for tree and building in the output of the cityscapes photos->labels task.CitationIf you use this code for your research, please cite our paper:@inproceedings{CycleGAN2017, title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networkss}, author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A}, booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on}, year={2017}}Related Projects:pix2pix: Image-to-image translation using conditional adversarial netsiGAN: Interactive Image Generation via Generative Adversarial NetworksCat Paper CollectionIf you love cats, and love reading cool graphics, vision, and ML papers, please check out the Cat Paper Collection.AcknowledgmentsCode borrows from pix2pix and DCGAN. The data loader is modified from DCGAN and Context-Encoder. The generative network is adopted from neural-style with Instance Normalization.© 2020 GitHub, Inc.TermsPrivacySecurityStatusHelpContact GitHubPricingAPITrainingBlogAbout You can’t perform that action at this time. You signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session."
9,語言支援  |  Cloud Speech-to-Text API  |  Google Cloud,https://cloud.google.com/speech-to-text/docs/languages?hl=zh-TW,"Google CloudLanguage Deutsch  English  Español (América Latina)  français  Português Brasileiro  日本語  简体中文  繁體中文  한국어 控制台  登入   AI & Machine Learning Products      Cloud 語音轉文字      說明文件    AI 與機器學習產品  與銷售人員聯絡  免費開始使用  語言支援 Cloud Speech-to-Text 使用可理解各種語言其中一種的語音辨識引擎。這些語言會在辨識要求的 languageCode 參數中指定。每個語言代碼參數都是由 BCP-47 ID 組成。這些標記的格式通常為「語言-地區」，其中的「語言」是指主要語言，「地區」則是指特定方言的地區 (通常為國家/地區 ID)。例如，英文可能以美式英文 (en-US) 或英式英文 (en-GB) 表示。Cloud 語音轉文字可以辨識下列語言的語音內容。語言languageCode語言 (英文名稱)Afrikaans (Suid-Afrika)af-ZA南非荷蘭文 (南非)አማርኛ (ኢትዮጵያ)am-ET阿姆哈拉文 (衣索比亞)Հայ (Հայաստան)hy-AM亞美尼亞文 (亞美尼亞)Azərbaycan (Azərbaycan)az-AZ亞塞拜然文 (亞塞拜然)Bahasa Indonesia (Indonesia)id-ID印尼文 (印尼)Bahasa Melayu (Malaysia)ms-MY馬來文 (馬來西亞)বাংলা (বাংলাদেশ)bn-BD孟加拉文 (孟加拉)বাংলা (ভারত)bn-IN孟加拉文 (印度)Català (Espanya)ca-ES加泰隆尼亞文 (西班牙)Čeština (Česká republika)cs-CZ捷克文 (捷克共和國)Dansk (Danmark)da-DK丹麥文 (丹麥)Deutsch (Deutschland)de-DE德文 (德國)English (Australia)en-AU英文 (澳洲)English (Canada)en-CA英文 (加拿大)English (Ghana)en-GH英文 (迦納)English (Great Britain)en-GB英文 (英國)English (India)en-IN英文 (印度)English (Ireland)en-IE英文 (愛爾蘭)English (Kenya)en-KE英文 (肯亞)English (New Zealand)en-NZ英文 (紐西蘭)English (Nigeria)en-NG英文 (奈及利亞)English (Philippines)en-PH英文 (菲律賓)English (Singapore)en-SG英文 (新加坡)English (South Africa)en-ZA英文 (南非)English (Tanzania)en-TZ英文 (坦尚尼亞)English (United States)en-US英文 (美國)Español (Argentina)es-AR西班牙文 (阿根廷)Español (Bolivia)es-BO西班牙文 (玻利維亞)Español (Chile)es-CL西班牙文 (智利)Español (Colombia)es-CO西班牙文 (哥倫比亞)Español (Costa Rica)es-CR西班牙文 (哥斯大黎加)Español (Ecuador)es-EC西班牙文 (厄瓜多)Español (El Salvador)es-SV西班牙文 (薩爾瓦多)Español (España)es-ES西班牙文 (西班牙)Español (Estados Unidos)es-US西班牙文 (美國)Español (Guatemala)es-GT西班牙文 (瓜地馬拉)Español (Honduras)es-HN西班牙文 (宏都拉斯)Español (México)es-MX西班牙文 (墨西哥)Español (Nicaragua)es-NI西班牙文 (尼加拉瓜)Español (Panamá)es-PA西班牙文 (巴拿馬)Español (Paraguay)es-PY西班牙文 (巴拉圭)Español (Perú)es-PE西班牙文 (秘魯)Español (Puerto Rico)es-PR西班牙文 (波多黎各)Español (República Dominicana)es-DO西班牙文 (多米尼克共和國)Español (Uruguay)es-UY西班牙文 (烏拉圭)Español (Venezuela)es-VE西班牙文 (委內瑞拉)Euskara (Espainia)eu-ES巴斯克文 (西班牙)Filipino (Pilipinas)fil-PH菲律賓文 (菲律賓)Français (Canada)fr-CA法文 (加拿大)Français (France)fr-FR法文 (法國)Galego (España)gl-ES加里西亞文 (西班牙)ქართული (საქართველო)ka-GE喬治亞文 (喬治亞)ગુજરાતી (ભારત)gu-IN古吉拉特文 (印度)Hrvatski (Hrvatska)hr-HR克羅埃西亞文 (克羅埃西亞)IsiZulu (Ningizimu Afrika)zu-ZA祖魯文 (南非)Íslenska (Ísland)is-IS冰島文 (冰島)Italiano (Italia)it-IT義大利文 (義大利)Jawa (Indonesia)jv-ID爪哇文 (印尼)ಕನ್ನಡ (ಭಾರತ)kn-IN卡納達文 (印度)ភាសាខ្មែរ (កម្ពុជា)km-KH高棉文 (柬埔寨)ລາວ (ລາວ)lo-LA寮文 (寮國)Latviešu (latviešu)lv-LV拉脫維亞文 (拉脫維亞)Lietuvių (Lietuva)lt-LT立陶宛文 (立陶宛)Magyar (Magyarország)hu-HU匈牙利文 (匈牙利)മലയാളം (ഇന്ത്യ)ml-IN馬拉雅拉姆文 (印度)मराठी (भारत)mr-IN馬拉地文 (印度)Nederlands (Nederland)nl-NL荷蘭文 (荷蘭)नेपाली (नेपाल)ne-NP尼泊爾文 (尼泊爾)Norsk bokmål (Norge)nb-NO挪威博克馬爾文 (挪威)Polski (Polska)pl-PL波蘭文 (波蘭)Português (Brasil)pt-BR葡萄牙文 (巴西)Português (Portugal)pt-PT葡萄牙文 (葡萄牙)Română (România)ro-RO羅馬尼亞文 (羅馬尼亞)සිංහල (ශ්රී ලංකාව)si-LK錫蘭文 (斯里蘭卡)Slovenčina (Slovensko)sk-SK斯洛伐克文 (斯洛伐克)Slovenščina (Slovenija)sl-SI斯洛維尼亞文 (斯洛維尼亞)Urang (Indonesia)su-ID巽他文 (印尼)Swahili (Tanzania)sw-TZ斯瓦希里文 (坦尚尼亞)Swahili (Kenya)sw-KE斯瓦希里文 (肯亞)Suomi (Suomi)fi-FI芬蘭文 (芬蘭)Svenska (Sverige)sv-SE瑞典文 (瑞典)தமிழ் (இந்தியா)ta-IN泰米爾文 (印度)தமிழ் (சிங்கப்பூர்)ta-SG泰米爾文 (新加坡)தமிழ் (இலங்கை)ta-LK泰米爾文 (斯里蘭卡)தமிழ் (மலேசியா)ta-MY泰米爾文 (馬來西亞)తెలుగు (భారతదేశం)te-IN泰盧固文 (印度)Tiếng Việt (Việt Nam)vi-VN越南文 (越南)Türkçe (Türkiye)tr-TR土耳其文 (土耳其)اردو (پاکستان)ur-PK烏都文 (巴基斯坦)اردو (بھارت)ur-IN烏都文 (印度)Ελληνικά (Ελλάδα)el-GR希臘文 (希臘)Български (България)bg-BG保加利亞文 (保加利亞)Русский (Россия)ru-RU俄文 (俄羅斯)Српски (Србија)sr-RS塞爾維亞文 (塞爾維亞)Українська (Україна)uk-UA烏克蘭文 (烏克蘭)עברית (ישראל)he-IL希伯來文 (以色列)العربية (إسرائيل)ar-IL阿拉伯文 (以色列)العربية (الأردن)ar-JO阿拉伯文 (約旦)العربية (الإمارات)ar-AE阿拉伯文 (阿拉伯聯合大公國)العربية (البحرين)ar-BH阿拉伯文 (巴林)العربية (الجزائر)ar-DZ阿拉伯文 (阿爾及利亞)العربية (السعودية)ar-SA阿拉伯文 (沙烏地阿拉伯)العربية (العراق)ar-IQ阿拉伯文 (伊拉克)العربية (الكويت)ar-KW阿拉伯文 (科威特)العربية (المغرب)ar-MA阿拉伯文 (摩洛哥)العربية (تونس)ar-TN阿拉伯文 (突尼西亞)العربية (عُمان)ar-OM阿拉伯文 (阿曼)العربية (فلسطين)ar-PS阿拉伯文 (巴勒斯坦國)العربية (قطر)ar-QA阿拉伯文 (卡達)العربية (لبنان)ar-LB阿拉伯文 (黎巴嫩)العربية (مصر)ar-EG阿拉伯文 (埃及)فارسی (ایران)fa-IR波斯文 (伊朗)हिन्दी (भारत)hi-IN北印度文 (印度)ไทย (ประเทศไทย)th-TH泰文 (泰國)한국어 (대한민국)ko-KR韓文 (南韓)國語 (台灣)zh-TW中文，華語 (繁體，台灣)廣東話 (香港)yue-Hant-HK中文，粵語 (繁體，香港)日本語（日本）ja-JP日文 (日本)普通話 (香港)zh-HK中文，華語 (簡體，香港)普通话 (中国大陆)zh中文，華語 (簡體，中國)  本頁內容對您是否有任何幫助？請提供意見： Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see our Site Policies. Java is a registered trademark of Oracle and/or its affiliates.  上次更新：十二月 5, 2019。 傳送您對下列選項的寶貴意見...  這個網頁   文件意見  Cloud 語音轉文字說明文件   產品意見    需要協助嗎？請前往我們的支援網頁。 "
10,[電腦視覺] 如何應用 Cloud AutoML Vision 辨識怪盜基德、工藤新一、服部平次 – 沒一村生活點滴,https://noootown.wordpress.com/2018/07/25/apply-cloud-automl-vision-distinguish-konan/,"跳至內容區沒一村生活點滴Watch me and look at me電腦視覺[電腦視覺] 如何應用 Cloud AutoML Vision 辨識怪盜基德、工藤新一、服部平次2018 年 07 月 25 日2018 年 07 月 25 日 沒一村 資料準備首先我們用 Google Search 來收集這三位角色的圖片作為訓練資料，這邊可以利用 google-images-download 這個 tool 批次下載圖片。如下圖，將下載的圖片分別整理至三個角色各自的資料夾中。Cloud AutoML Vision 有兩種上傳訓練圖片的方式，這邊我們使用 Cloud Storage 匯入。首先，將收集好的訓練圖片上傳至 Cloud Storage (alpha 有限定要在 PROJECT_ID-vcm 這個 bucket 裡面，假如你的 project ID 是 test-123 那麼就要上傳到 test-123-vcm 這個 bucket)。如下圖：接著要建立一個 CSV 檔案描述訓練圖片 URL 和其所對應的 labels，CSV 內容節錄部分如下：gs://-vcm/Conan/HarleyHartwell/98.jpg,HarleyHartwellgs://-vcm/Conan/HarleyHartwell/99.jpg,HarleyHartwellgs://-vcm/Conan/JimmyKudo/1.jpg,JimmyKudogs://-vcm/Conan/JimmyKudo/10.jpg,JimmyKudo然後將該 CSV 也上傳到 Cloud Storage 中，如下：創建 Dataset在 Cloud AutoML Vision 的 console 中，點選 「NEW DATASET」。填入 Dataset 名稱，指定好 CSV 的 Cloud Storage URL，點選 「Create Dataset」(訓練資料如果很多的話，需要稍等片刻等待資料匯入完成)。匯入完成後，可以在「IMAGES」tab 可以看到匯入圖片的縮圖。模型訓練與評估在「TRAIN」tab 點選 「TRAIN NEW MODEL」按鈕選擇 Training budget (理論上 compute hour 愈多，訓練出來的 model 準確率越高)，然後按下 「START TRAINING」按鈕 (這通常需要一些時間)沒錯，你完全不需要具備 machine learning 的背景知識，就可以訓練出一個 machine learning model。訓練完成後，可以在「EVALUATE」tab 看到訓練的結果，包括 Precision、Recall、Confusion matrix 等指標 (參照 lee 文)利用模型預測到這邊我們已經完成了這個柯南角色辨識器啦！切到「PREDICT」tab 上傳一張你想要辨識的圖片，這個辨識器就會告訴你這個角色是誰，並且有一個信心分數 (0 ~ 1)。頁面下方也會給出 prediction API 的使用範例。你不需要擔心這個 API 是 host 在哪邊也不需要擔心 scaling 的問題，Cloud AutoML 會幫你代管這個 API 服務。持續迭代優化模型準確率你也許會問，有什麼方式可以提升 model 的準確率呢？在 Cloud AutoML 當中，因為 training 和 evaluation 是由 Cloud AutoML 自動處理，因此我們只能藉由提升訓練資料的品質，來提升模型的準確率。切換至 「IMAGES」tab，我們發現在 JimmyKudo 的訓練資料當中，其實有些圖片並不是 JimmyKudo，因此我們可以將這些錯誤的訓練資料刪除，提升訓練資料的品質，如下圖。整理完各類別的訓練資料之後，我們再重新訓練一個新的 model，準確率果然有顯著的提升。Cloud AutoML 獨家搶先體驗 結合 AI 大眾化的趨勢，Google Cloud 首席合作夥伴：GCP專門家架設了「Cloud AutoML 獨家體驗專區」，讓所有人都能即刻感受 Cloud AutoML 的威力。若想客製化擁有自己的 Cloud AutoML 模型，GCP專門家提供以下教學文章與應用案例：[手把手教學] 快速啟用 Cloud AutoML Vision：Google 最新機器學習產品！如何應用 Cloud AutoML Vision 辨識屈中恆、宋少卿、鈕承澤！Google 三大機器學習產品比較擁有專屬自己的機器學習模型想立即擁有自己的客製化機器學習模型嗎？想訓練模型卻不知從何下手嗎？立刻與 GCP專門家聯繫吧！瀏覽更多 Cloud AutoML 相關文章與 Google Cloud 產品應用，詳見 GCP專門家技術部落格，最新知識均在此與您分享。 分享此文：TwitterFacebook請按讚：讚 載入中...相關 分享此文：TwitterFacebook請按讚：讚 載入中... computer vision Google Cloud Platform Machine Learning 工商 發表迴響 取消回覆 在此輸入你的回應…Please log in using one of these methods to post your comment:     電子郵件 (必) （電子郵件地址不會被公開）名稱 (必)個人網站您的留言將使用 WordPress.com 帳號。( 登出 / 變更 )您的留言將使用 Google 帳號。( 登出 / 變更 )您的留言將使用 Twitter 帳號。( 登出 / 變更 )您的留言將使用 Facebook 帳號。( 登出 / 變更 )取消連結到 %s 透過電子郵件通知我後續回應。 有新文章時用Email通知我。 FacebookGithubLinkedInSponsor 我喜歡寫一些文章，幫助對網頁入門有興趣的朋友一把，如果我的文章啟發您對網頁前端的興趣，我很高興能有這樣的成果。以下是贊助的連結，如果我的文章曾經幫助過您，請參考您的經濟狀況，決定要不要支持我的創作：）贊助沒一村Fan Page Fan PageSearch搜尋：Popular [資工雜筆] awk 好用用法整理 [前端連載] 買早餐也能了解前端 vs 後端？ [程設雜筆] 試玩 CSS custom scrollbar [點點滴滴] 寫網頁，很難嗎？ [點點滴滴] 心理學期末報告——讀書心得 CategoryCategory選擇分類程式設計雜筆  (39)自然語言處理  (2)電腦視覺  (2)資工問題雜筆  (30)資料科學  (2)點點滴滴  (17)前端連載  (8)我的作品  (4)機器學習  (1)交大修課心得  (12) Recent [資工雜筆] 資料庫和分散式系統雜筆[資工雜筆] Linux 網管雜筆[電腦視覺] 如何應用 Cloud AutoML Vision 辨識怪盜基德、工藤新一、服部平次[程設雜筆] Golang 雜筆[資工雜筆] bash command 雜筆Hit 517,132 hitsTagAJAXAndroidapp設計awkBashcommand linecomputer visionCSSDeep LearningECMAScript6Git&GithubGoogle Cloud PlatformHTMLJavaJavascriptJqueryLinuxMachine LearningNatural Language ProcessingOS XpythonReactJsR languageShell scriptTensorflowUbuntuUI&UXVimVim scriptvirtual machine交大修課心得交通大學初體驗前端vs後端創業實習工商我的作品正規語法瘋事社團程式語言網路網路爬蟲網頁設計色彩學計算機概論資工課遊戲體悟RSS用 RSS 訂閱最新文章 在 WordPress.com 建立免費網站或網誌.佈景主題：Button，發表者：Automattic。張貼到取消 隱私權與 Cookie：此網站可使用 Cookie。繼續使用此網站即表示你同意使用 Cookie。若要瞭解更多資訊，包括如何控制 Cookie，請參閱此處：Cookie 政策 %d 位部落客按了讚：%d 位部落客按了讚："
